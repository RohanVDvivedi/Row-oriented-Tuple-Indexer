new tasks:: (yet to be prioritized and ordered)
 * segregate insert and delete into 2 functions, to reuse code, provide implementations to split_insert function call and merge_if_possible
 * locked_page_info may have READ_LOCKED/WRITE_LOCKED and MODIFIED/UN_MODIFIED bit fields to support the above task
 * build stacked iterator using bplus_tree_locked_pages_stack

 * (minisclue optimization) in may_require_split_for_insert calculate new_tuple_available_space = (allotted_space - used_space), and compare it with tuple->size (fixed length tuple) and (allotted_space / 2) (for varibale length tuple maximum size), to check if the page may split

 * make bplus_tree_locked_pages_stack into locked_pages_stack, since this stack can be used for any further data structure

 * implement bplus_tree_cursor
 * implement find
   * implement TOWARDS_FIRST_RECORD_WITH_KEY and TOWARDS_LAST_RECORD_WITH_KEY in interior page util functions

 * implement extract key from record and index tuples in bplus_tree_tuple_definitions
 * implement building index tuple from record tuple (and key tuple) and page_id in bplus_tree_tuple_definitions

 * implement update which split or merges depending on the size of the node being updated,
   it does nothing for a fixed length tuple updates
   * inplace updates of fixed length record b+tree is very simple and highly parallelizable, we only need read locks on interior pages and write lock only on the leaf page (and on the root page, so that we are sure that root is not a leaf and we can safely read the level of the tree)
   * you my want to segregate functionalities of insert and delete into different functions, to accomodate this

 * implement delete range, deleting an entire sub tree if it falls into the range

 * ponder on supporting duplicates, by assigning a number sequence to each duplicates, i.e.
   * for 2 duplicates "key"->"value_A" and "key"->"value_B", we make them into "key",0->"value_A" and "key",1->"value_B"
   * we increment the sequence for a key every time we find the last one
   * for deletes they need to provide the 

 * provide a way to allow additional default header in the implementation

 * implement find test cases to find range of values, add this tests between insert and delete test cases

 * test insertion of tuple longer than max insertable record size
 * test deletion of non existing keys
 * test inserting records with already existing keys
 * interleave insertions and deletion test cases

 * implement redistribute keys functions for fixed length index_def and record_def tuples, this will reduce propogation of merges, this task can be delayed to be done at the end

 * implement an error handling strategy to check the was_modified parameter, in in-memory-data-store, and exit with error, if the page was modified and was_modified was not set OR if page was not modified and was_modified was set
 * go through the project and implemnt better error handling for failures of malloc and data_access_methods
 * implement throwing errors appropriately, upon failures from dam_p

 * write multi threaded test cases to insert a large csv in to b+tree, measure performance difference
