** PICKED TASKS

VALIDATING WAS_MODIFIED BIT
 * implement an error handling strategy to check the was_modified bit, in in-memory-data-store, and exit with error, if the page was modified and was_modified was not set.

TRANSACTION_CONTEXT task
 * struct transaction_abort_manager {
    void* transaction_handle; // user defined type to represent the transaction
    const transaction_abort_manager_methods* tamm_p;
 };
 struct transaction_abort_manager_methods{
    const void* context;
    int is_aborted(const void* transaction_id, const void* context); // returns 0 if not aborted, else returns reason for the abort
    void mark_aborted(const void* transaction_id, const void* context, int reason); // abort the transaction, with a reason for abort
 };
 * make an opaque_structs for transaction_abort_manager_methods and use only below methods in the TupleIndexer project
 *  int is_aborted(const transaction_abort_manager* tamm_p);
    void mark_aborted(const transaction_abort_manager* tamm_p, int reason);
 * mark_aborted appends a abort log entry and flushes the WAL
 * make all pmm methods to take transaction_handle as input
 * make all dam methods to take transaction_abort_manager as the input parameter
 * make all persistent_page methods (that are wrappers to pmm and dam) to take transaction_abort_manager as input parameter, and pass respective parameter down, to wrapped methods
 * for an aborted transaction anywhere in TupleIndexer, release all persistent_page locks using dam_p and exit, undo operation has to be taken care of by the user
 * after each call to dam method wrappers for persistent_page, check if the transaction is aborted, if so release all locks and exit.

DATA_ACCESS_METHODS with tuple level locking
 * allow data_access_methods (with a context) to also enable locking only tuples on the page
 * function check if a tuple on a page is locked

DUPLICATES ALLOWING FOR INSERTS
 * add a flag as a parameter "fail_on_duplicate_key", that decides to fail an insert if a record being inserted has a duplicate key, else allow duplicates

ERROR HANDLING
 * elaborate error return in bplus_tree functions to acknowledge below
   (SUCCESS | FAILURE), OUT_OF_RESOURCES, ROLLBACK_REQUIRED as an int bit map

ERROR RETURN CODES
 * suggesting reason of a failure of data_access_method functions
 * reasons being DEADLOCK_HANDLED, CAN_T_LOCK_FREE_PAGE, OUT_OF_DISK_SPACE, OUT_OF_FREE_BUFFERS

** UTILITY
 * segregate insert and delete both into 2 functions, to reuse code, provide implementations to split_insert function call and merge_if_possible
 * build stacked iterator using locked_pages_stack, this can be used to iterate over all the tuples in the b+tree including the interior page and also only index tuples

** UPDATE FUNCTIONALITY
 * implement update which split or merges depending on the size of the record being updated, (and does neither for fixed length records or if new record and old record are of same size)
 * update by key (updates the last 1) or update by record (updates an exact match record -> here we will already know if it will be a merge or a split)

** ADDITIONAL DELETE FUNCTIONALITY
 * implement delete range, deleting an entire sub tree if it falls into the range (parameter key1 and key2, (key1 <= key2) and flags that suggest whether key1 and key2 are inclusive or not)
 * implement delete record, to delete a specific record from b+tree

** OPTIMIZATION
 * implement redistribute keys functions for fixed length index_def, this will reduce propogation of merges, this task can be delayed to be done at the end
 * see how this can be accomodated in the delete and update functions available at that point in future

** NECESSARY BUT DELAYED UNTIL ACTUAL REQUIREMENTS ARE FULFILLED
 * go through the project and implemnt better error handling for failures of malloc and data_access_methods
 * write multi-threaded test cases to insert a large csv in to b+tree, measure performance difference

** OTHER DATABASE DATASTRUCTURE TASKS (VERY IMPORTANT FOR FUTURE OF PROJECT)
 * linked_page_list push_tuple_at_head, push_tuple_at_tail, get_head_tuple, get_tail_tuple, pop_tuple_from_head and pop_tuple_from_tail as its access functions and an iterator from head to tail or the other way around. it will be a doubly linkedlist of tuples
 * all inserts to linked_page_list have a flag to PRESERVE_ROOT_PAGE_ID
 * hash_table, a linux page table like hashtable where each bucket points a linked_page_list, each working with PRESERVE_ROOT_PAGE_ID
 * sort that can be called on a linked_page_list or a hash_table
 * building a b+tree using a sorted linked_page_list, layer by layer - think how to get it done without having first index entries of child interior pages
