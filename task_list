** PICKED TASKS

DAM FEATURES (CONTIGUOUS ALOCATION and DEALLOCATION of pages) (Still think if you need this, with current hashtable design we have, we do not need this)
  * uint64_t allocate_contiguous_pages(uint64_t page_count), returning first page id of the allocated pages.
  * free_contiguous_pages will not be supported, since we need to be aware that the page must not be locked or waiting to be locked, while doing this, and so we would need a lot to do to undo in case if a page can not be freed, hence only allocate_contiguous_pages will be the only thing close to what we will support in dam api.

** OTHER DATABASE DATASTRUCTURE TASKS (VERY IMPORTANT FOR FUTURE OF PROJECT)
 * bplus_tree bulk loading
   * bulk loading to accept bpttd_p, dam_p and pmm_p and allocate root page first
   * api to accept accept records, tuple by tuple
   * on build_call, generate interior pages
   * for building ith level of interior page, from the i-1th level, we copy the 1st row from the page with page id of the page, we will use the least_keys_page_id to link the level of the interior pages. for building tuple for iterior page level 1, we will use the suffix compression also to build the index tuple, while for other levels we will only use the same index tuple as in the first row of the interior page.
   * in the last pass, we will revisit all the interior pages using dfs and move the first row's page_id to least_keys_page_id and discard the first row.
 * a page_table that maps a 64 bit integer to a page_id
   * It is structured as a tree, starting with level 0 at the leaf, and then increasing towards root, like bplus tree.
   * each page at any level is identical and contains list of page_ids, each of page_id_width, pointing to another page_table page (when its level > 0) OR something else (when its level = 0), all pages are identical.
   * If there are E entries in each of the page, then that page is bound to point to E^(n+1) buckets/entries in all.
   * A interior node with level n (n > 0), can have child with any level p (n > p >= 0).
   * with each node there is associated start_entry_id and a level n, each of that page's entries point to another valid subtree reach leaf entries in range, [start_entry_id * i * (E^n), start_entry_id * (i+1) * (E^n)), where is the slot number of ith entry on that page. It is also true that the range of slots that the parent can point to is [start_entry_id, start_entry_id + E^(n+1))
   * As shown in point above each node stores its level n and its own start_entry_id in the header.
   * child pages are allocated as and when required, not upfront
   * For instance, If the parent of the page has level n (n > q) with start entry id s, and the child at slot i has level q with start_entry_id p, then the child is allowed to expand to [s * i * (E^n), s * (i+1) * (E^n)). NOTE in this case it must be true that the range of the child [p, p + E^(q+1)) must fall within the range designated by the parent, ideally it is the range designated by the parent if (q == n-1) or a range of a child in its subtree (if it was fully populated).
   * locking a node means locking the designated range of the node, not its actual range. designated range is given by the parent, not the headers of the node itself.
   * a root node always has a deginated range of [0, 2^b) where b is the number of the bits used for entry_id
   * If any page becomes all empty, then it is freed and an entry from its parent is discarded, shrinking the page_table up the chain.
   * an entry_id X is pointed to by a page of level n in its subtree, with start_entry_id s, only if X lies in range [s, s + E^(n+1)). (here s is also a multiple of E^(n+1)) (The range given here, in this point is actual range not designated range, you can not get slot index for designated range.)
     * where s = floor(X / (E^(n+1))) * E^(n+1)
     * and the child slot pointing to X is at index i in the page where, i = floor((X - s) / (E^n)) = floor((X % (E^(n+1))) / (E^n))
   * api:
     * walk_down_page_table(uint64_t root_page_id, uint64_t start_entry_id, uint64_t last_entry_id, int leaf_lock_type (READ_LOCK or WRITE_LOCK), int create_if_absent);
       * returns a struct that stores designated range given by the parent of the locked page, and a locked pages stack enough to hold any subtree of the range
       * walks down latch coupling taking write locks, releasing parent locks when possible
       * you can release parent lock, if there is only 1 entry in the parent points to a sub tree such that the designated range of that subtree completely encompasses the proposed query range
       * we get this by checking the slot index for the start_entry_id and last_entry_id, if they are same then it is safe to release parent lock. 
       * returns when the range requested lies completely within the designated range for a node, with more than 1 entries pointing to the subtrees in the range.
       * if a page in the walk down does not exists then fail, if create_if_absent = 0, else create interior pages and leaves as required
     * set_entry_page_id and get_entry_page_id, to get and set values on the struct returned by the above struct
       * they will use the designated range as the range that can be accessed, and the locked page as the root.
       * they will create nodes and increase level of the nodes, rooted at the given locked node, as they like
       * get will return NULL_PAGE_ID if the entry does not exists, while set can create and destry nodes and entries at will.
     * shrink_page_table(uin64_t root_page_id, uint64_t entry_id);
       * to be called when you write NULL_PAGE_ID to entry_id, it will shrink the page_table taking write locks all the way until leaf and freeeing pages on the path on the way up, if the page only consists of NULL_PAGE_ID's in it
       * at the end if the root only has 1 entry at its first entry, then the contents of its only leaf are copied onto it, shrinking the tree up a level.
       * think about optimizations like, when there are more than 1 valid non NULL entries in interior or leaf page, then release all parent locks.
     * release_locks function to release page_table lock acquired by the walk_down_page_table function.
     * check_is_root_locked
       * check if we locked the root page
 * linked_page_list a doubly linkedlist of pages with tuple on them
   * all insert/update/delete functions take flag to FIXED_HEAD, to always keep head at the same page_id, moving it's next contents to it if the head page becomes empty
   * read and write iterator allowing you to update/delete in place
   * insert can take falgs like AT_HEAD, AT_TAIL, ANYWHERE -> to insert any where on any page
   * on delete, any page that becomes empty is removed, unless its head with FIXED_HEAD flag set
   * this can work as both stack and queue
   * tuples are never moved, any page other than head page, can be discarded if it becomes empty
   * deletes to other tuples (other than head and tail) are allowed by updating them with NULL, using a writable iterator
   * a writable iterator can write to an existing slot if it is NULL and if that tuple fits on the page
 * hash_table, a page_table points a linked_page_list
   * with (duplicate keys allowed)
   * when a linked_page_list becomes all empty, then linked_page_list is destroyed and NULL_PAGE_ID is written to page_table
     * we let the page_table mnage shrinking, keeping the root_page at fixed page_id
 * linked_page_list push_tuple_at_head, push_tuple_at_tail, get_head_tuple, get_tail_tuple, pop_tuple_from_head and pop_tuple_from_tail as its access functions and an iterator from head to tail or the other way around. it will be a doubly linkedlist of tuples
 * all inserts to linked_page_list have a flag to PRESERVE_ROOT_PAGE_ID
 * sort
   * it accepts record in the bplus_tree_bulkloading like interface
   * puts tuples in a page, sorts them and inserts it into a page_table
   * it will then merge adjacent runs until the page_table contains only 1 entry
 * Rtree will be supported
   * very similar to b+tree, n dimensional data can be searched, inserted, deleted, no least_keys_page_id required
   * splits and merges happen according to the minimal increase in interior node areas
   * searches walk down a tree, and maintain a stack, releasing locks only if only 1 entry is to be scannd in the interior node, next, prev and get_curr are supported, leafs may be acquired with write locks
   * inserts walk down only 1 path of the tree, trying to minimize the increased area
   * deleted walk down like a search would, and since we maintain a stack, a delete can be performed, which will destroy the stack releasing all locks
   * you need to define a rtree_tuple_definitions, that also has a function to get the hyper_rectangle for any of the records, which defines the minimum area hyper rectangle that the record falls into, as expected that allows you to store n dimensional lines, points etc.
   * hyper rectangle is an n dimensional rectangle.
   * all interior nodes store a hyper rectangle and a corresponding page_id, and atleast 2 entries must fit on a page, else rttd contruction fails. yet you can have any number of dimensions.
   * you can also specify any numeral datatype for dimension, as long as it is supported by the TupleStore.
   * need to think about how to calculate areas of the hyper rectangle with dimensions of different types, such that it fits a number and also does not violate precision requirements (getting areas is necessary for splitting and merging). think about using GNU MP for this purpose of getting area. BUT we can not store GNU MP number in the TupleStore. ALSO you can just use GNU MP for substractions of lower_bound and upper_bound to estimate splits and merges, but while constructing bounding boxes, you can revert to get_max(d0_lb_1, d0_lb_1), etc for computing the new bounding boxes.
   * fail insertions if the lower_bound and upper_bound of the get_hyper_rectangle do not follow <= comparison for any of the n dimensions.
   * build utility function to check that hyper rectangle does not have a negative dimension (i.e. lower_bound <= upper_bound), construct hyper_rectangle covering 2 or more hyper rectangles, and check if a hyper_rectangle intersects with another hyper_rectangle etc.
   * all queries take only hyper_rectangle keys for search and delete, while user takes a leaf record (this is why we need a function to build a hyper rectangle).
   * inserts and deletes can release page locks if it is deemed that split and merge would not propogate up the tree
   * accomodate any tuple that is atleast half the size of the leaf page, and interior page tuples would be fixed width with non NULL elements
   * keys will have a even count and all of numeral types, for ith dimension lower bound would be 2*i th key element and 2*i+1 will be upper bound, up to d dimentions (0 <= i < 2*d).
 * think about implementing more data structures like T-tree, Patricia Trie, etc

** OPTIMIZATION (THIS MAY NEVER MAKE IT TO THE PROJECT)
 * implement redistribute keys functions for fixed length index_def, this will reduce propogation of merges, this task can be delayed to be done at the end
 * prefix key compression -> store prefix compression for var_strings (for internal nodes only), working in the following ways
    0  ->  "ABCD"            "ABCD"
    1  ->  "ABDEF"           "2DEF"
    2  ->  "ABDFG"           "3FG"
    3  ->  "ABDH"       ->   "3H"
    4  ->  "ADEA"            "1DEA"
    5  ->  "ADEF"            "3F"
    6  ->  "ADEGH"           "3GH"
    each var string except the first 1 stores itself OR its difference from the (same column) var string in the previous row
    the difference is stores as a number (suggesting the common characters from the previous var string) followed by the characters that are different.

    to differentiate the prefix number from the other characters, we will always store them in element_def's size_specifier_prefix_size number of bytes after an invalid unicode character "0xFF 0xFF".

    Algorithm to construct the string at ith index
    compute the length of the var string, that is 3 + "G" 1 + "H" 1 = 5
    bytes_to_read = 5 stack = ""
    now start iterating until bytes_to_read > 0
    i == 6 -> bytes_to_read = 3 stack = "HG"
    i == 5 -> bytes_to_read = 3 stack = "HG"
    i == 4 -> bytes_to_read = 1 stack = "HGED"
    i == 3 -> bytes_to_read = 1 stack = "HGED"
    i == 2 -> bytes_to_read = 1 stack = "HGED"
    i == 1 -> bytes_to_read = 1 stack = "HGED"
    i == 0 -> bytes_to_read = 0 stack = "HGEDA" -> reverse "ADEGH"

    stack = ""
    bytes_to_read = 5
    i = 6
    while(bytes_to_read > 0)
    {
      common_char_count, rest_of_string = get_column_from_ith_row(i);
      new_bytes_to_read = min(bytes_to_read, common_char_count);
      if(new_bytes_to_read > bytes_to_read)
      {
        for(int i = (new_bytes_to_read - bytes_to_read) - 1; i >= 0; i--)
          stack.push(rest_of_string[i]);
      }
      bytes_to_read = new_bytes_to_read;
    }
    return stack.reverse();

* think about future project (THIS WILL BE PROJECTS THAT WILL DEPEND ON THIS PROJECT)
 * think about how to do undo and redo logging
 * logical logging with physiological logging will allow us higher concurrency
 * mini transactions like that innodb can generate very large log records, which we may not want in memory
   * clubbing all physiological logs for a logical operation into a single buffer and log them only upon completion (with strict 2pl on the latches), can use large amount of memory for log buffer, but you can easily redo and undo it, it is never undone physiologically, only logically
 * How about we use logical mini transaction redo, composed of physiological logs, now we do physiological undo when logical operation is incomplete, and logical undo for other operations above. we may have to undo and undo in this case
