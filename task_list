** PICKED TASKS

 * discard FORCE_FLUSH from dam options, tuple indexer will never ask you to force flush
 * rename in_memory_data_store to unWALed_in_memory_data_store
 * after an abort, all release_lock calls without FREE_PAGE option must pass, if it fails, printf("BUG :: release_lock failed after an abort"); and exit

 * Avoid malloc call, while merging interior pages, instead of duplicating the separator_tuple, insert separator tuple, and then update its child_page_id, this will avoid 1 malloc altogether

 * implement an error handling strategy to check the was_modified bit, in in-memory-data-store, and exit with error, if the page was modified and was_modified was not set.

 * dam in release_lock(FREE_PAGE) -> allow the operation only if there is exactly only 1 thread, with reader and writer lock on the page
 * you are allowed to release lock on the page and free it simultaneously, if you are the only 1 who has lock on it
 * dam functions add functionality to allow allocating and freeing pages given by page count, uint64_t allocate_contiguous_pages(uint64_t page_count), returning first page id of the allocated pages.
 * free_contiguous_pages(uint64_t first_page_id, uint64_t page_count), instead of free_page -> to free n contiguous pages.
 * free_page must fail with an abort, if there are concurrent readers OR writers to the page, i.e. if a page is read or write locked (allow only 1 locker for the calling thread if FREE_PAGE is set for release_lock), while freeing a page.

** OTHER DATABASE DATASTRUCTURE TASKS (VERY IMPORTANT FOR FUTURE OF PROJECT)
 * linked_page_list push_tuple_at_head, push_tuple_at_tail, get_head_tuple, get_tail_tuple, pop_tuple_from_head and pop_tuple_from_tail as its access functions and an iterator from head to tail or the other way around. it will be a doubly linkedlist of tuples
 * all inserts to linked_page_list have a flag to PRESERVE_ROOT_PAGE_ID
 * hash_table, a linux page table like hashtable where each bucket points a linked_page_list, each working with PRESERVE_ROOT_PAGE_ID
 * sort that can be called on a linked_page_list or a hash_table
 * utility header function that reads page_type from common_page_header and helps get and set the next_page_id and prev_page_id
   * this can be used to build a generic iterator over linked list, bplus_tree leaf pages and interior pages
 * building a b+tree using a sorted linked_page_list, layer by layer
   * we will first ask for the sorted page double linked list, then build the bplus_tree level by level (level 0 being leaf page)
     * for building ith level of interior page, we copy the first tuple's key of all the lower level (i-1 level) pages, and build a singly linkedlist of interior pages composed of each index record pointing to the lower level page, we maintain only the heads of the 2 adjacent levels and a write iterator to the upper level and read iterator of the lower level.
     * since in the initial phase we are not storing the least_keys_page_id in the interior pages, we can use this interior page header field to link all pages in the same level as a singly linkedlist
     * for this we need to define an union {all_least_keys_page_id next_page_id}, to use the utility iterator to work.

** OPTIMIZATION
 * implement redistribute keys functions for fixed length index_def, this will reduce propogation of merges, this task can be delayed to be done at the end
 * prefix key compression -> store prefix compression for var_strings (for internal nodes only), working in the following ways
    0  ->  "ABCD"            "ABCD"
    1  ->  "ABDEF"           "2DEF"
    2  ->  "ABDFG"           "3FG"
    3  ->  "ABDH"       ->   "3H"
    4  ->  "ADEA"            "1DEA"
    5  ->  "ADEF"            "3F"
    6  ->  "ADEGH"           "3GH"
    each var string except the first 1 stores itself OR its difference from the (same column) var string in the previous row
    the difference is stores as a number (suggesting the common characters from the previous var string) followed by the characters that are different.

    to differentiate the prefix number from the other characters, we will always store them in element_def's size_specifier_prefix_size number of bytes after an invalid unicode character "0xFF 0xFF".

    Algorithm to construct the string at ith index
    compute the length of the var string, that is 3 + "G" 1 + "H" 1 = 5
    bytes_to_read = 5 stack = ""
    now start iterating until bytes_to_read > 0
    i == 6 -> bytes_to_read = 3 stack = "HG"
    i == 5 -> bytes_to_read = 3 stack = "HG"
    i == 4 -> bytes_to_read = 1 stack = "HGED"
    i == 3 -> bytes_to_read = 1 stack = "HGED"
    i == 2 -> bytes_to_read = 1 stack = "HGED"
    i == 1 -> bytes_to_read = 1 stack = "HGED"
    i == 0 -> bytes_to_read = 0 stack = "HGEDA" -> reverse "ADEGH"

    stack = ""
    bytes_to_read = 5
    i = 6
    while(bytes_to_read > 0)
    {
      common_char_count, rest_of_string = get_column_from_ith_row(i);
      new_bytes_to_read = min(bytes_to_read, common_char_count);
      if(new_bytes_to_read > bytes_to_read)
      {
        for(int i = (new_bytes_to_read - bytes_to_read) - 1; i >= 0; i--)
          stack.push(rest_of_string[i]);
      }
      bytes_to_read = new_bytes_to_read;
    }
    return stack.reverse();