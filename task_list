** PICKED TASKS

 * test cloning for find on hash_table_iterator

** OTHER DATABASE DATASTRUCTURE TASKS (VERY IMPORTANT FOR FUTURE OF PROJECT)
 * spend a day on output of `grep -inr --include "*.c" -A 5 -B 5 ",\sabort_error);" ./src > tb` and ensure that all aborts are handled properly
 * convert malloc calls, where ever possible to stack allocations (with or without alloca calls)
 * implement redistribute keys functions for fixed length index_def, this will reduce propogation of merges, this task can be delayed to be done at the end
   * this will be supported only for fixed_length index entries
   * function will move only 1 leaf/index entry from left to right or right to left at a time, everything will be dome in place
   * fails if the receiving page is out of space, easy peasy with unaltered and altered util functions
   * parent page will also be required and will be updated immediately in the call, all 3 pages, parent and 2 children will be provided as is
   * will fail, also if index entries are not fixed sized
   * separate function to be worked out for leaf and index pages
   * also we will call this, as rotation and we need functions to check if we can left and right rotate
 * make functions to get the maximum pages that will be locked at once for any of the TupleIndexer data structures and iterators
 * Rtree will be supported
   * very similar to b+tree, n dimensional data can be searched, inserted, deleted, no least_keys_page_id required
   * splits and merges happen according to the minimal increase in interior node areas
   * searches walk down a tree, and maintain a stack, releasing locks only if only 1 entry is to be scannd in the interior node, next, prev and get_curr are supported, leafs may be acquired with write locks
   * inserts walk down only 1 path of the tree, trying to minimize the increased area
   * deleted walk down like a search would, and since we maintain a stack, a delete can be performed, which will destroy the stack releasing all locks
   * you need to define a rtree_tuple_definitions, that also has a function to get the hyper_rectangle for any of the records, which defines the minimum area hyper rectangle that the record falls into, as expected that allows you to store n dimensional lines, points etc.
   * hyper rectangle is an n dimensional rectangle.
   * all interior nodes store a hyper rectangle and a corresponding page_id, and atleast 2 entries must fit on a page, else rttd contruction fails. yet you can have any number of dimensions.
   * you can also specify any numeral datatype for dimension, as long as it is supported by the TupleStore.
   * need to think about how to calculate areas of the hyper rectangle with dimensions of different types, such that it fits a number and also does not violate precision requirements (getting areas is necessary for splitting and merging). think about using GNU MP for this purpose of getting area. BUT we can not store GNU MP number in the TupleStore. ALSO you can just use GNU MP for substractions of lower_bound and upper_bound to estimate splits and merges, but while constructing bounding boxes, you can revert to get_max(d0_lb_1, d0_lb_1), etc for computing the new bounding boxes.
   * fail insertions if the lower_bound and upper_bound of the get_hyper_rectangle do not follow <= comparison for any of the n dimensions.
   * build utility function to check that hyper rectangle does not have a negative dimension (i.e. lower_bound <= upper_bound), construct hyper_rectangle covering 2 or more hyper rectangles, and check if a hyper_rectangle intersects with another hyper_rectangle etc.
   * all queries take only hyper_rectangle keys for search and delete, while user takes a leaf record (this is why we need a function to build a hyper rectangle).
   * inserts and deletes can release page locks if it is deemed that split and merge would not propogate up the tree
   * accomodate any tuple that is atleast half the size of the leaf page, and interior page tuples would be fixed width with non NULL elements
   * keys will have a even count and all of numeral types, for ith dimension lower bound would be 2*i th key element and 2*i+1 will be upper bound, up to d dimentions (0 <= i < 2*d).
 * think about implementing more data structures like T-tree, Patricia Trie, etc

LEFT FOR LATER TASKS
DAM FEATURES (CONTIGUOUS ALOCATION and DEALLOCATION of pages) (Still think if you need this, with current hashtable design we have, we do not need this)
  * uint64_t allocate_contiguous_pages(uint64_t page_count), returning first page id of the allocated pages.
  * free_contiguous_pages will not be supported, since we need to be aware that the page must not be locked or waiting to be locked, while doing this, and so we would need a lot to do to undo in case if a page can not be freed, hence only allocate_contiguous_pages will be the only thing close to what we will support in dam api.

** OPTIMIZATION (THIS MAY NEVER MAKE IT TO THE PROJECT)
 * prefix key compression -> store prefix compression for var_strings (for internal nodes only), working in the following ways
    0  ->  "ABCD"            "ABCD"
    1  ->  "ABDEF"           "2DEF"
    2  ->  "ABDFG"           "3FG"
    3  ->  "ABDH"       ->   "3H"
    4  ->  "ADEA"            "1DEA"
    5  ->  "ADEF"            "3F"
    6  ->  "ADEGH"           "3GH"
    each var string except the first 1 stores itself OR its difference from the (same column) var string in the previous row
    the difference is stores as a number (suggesting the common characters from the previous var string) followed by the characters that are different.

    to differentiate the prefix number from the other characters, we will always store them in element_def's size_specifier_prefix_size number of bytes after an invalid unicode character "0xFF 0xFF".

    Algorithm to construct the string at ith index
    compute the length of the var string, that is 3 + "G" 1 + "H" 1 = 5
    bytes_to_read = 5 stack = ""
    now start iterating until bytes_to_read > 0
    i == 6 -> bytes_to_read = 3 stack = "HG"
    i == 5 -> bytes_to_read = 3 stack = "HG"
    i == 4 -> bytes_to_read = 1 stack = "HGED"
    i == 3 -> bytes_to_read = 1 stack = "HGED"
    i == 2 -> bytes_to_read = 1 stack = "HGED"
    i == 1 -> bytes_to_read = 1 stack = "HGED"
    i == 0 -> bytes_to_read = 0 stack = "HGEDA" -> reverse "ADEGH"

    stack = ""
    bytes_to_read = 5
    i = 6
    while(bytes_to_read > 0)
    {
      common_char_count, rest_of_string = get_column_from_ith_row(i);
      new_bytes_to_read = min(bytes_to_read, common_char_count);
      if(new_bytes_to_read > bytes_to_read)
      {
        for(int i = (new_bytes_to_read - bytes_to_read) - 1; i >= 0; i--)
          stack.push(rest_of_string[i]);
      }
      bytes_to_read = new_bytes_to_read;
    }
    return stack.reverse();
 * OPTIMIZATION in suffix truncation :: handle cases if INT, UINT, LARGE_UINT, BIT_FIELD, in loop 1, if unequal on ASC-> then set element to last_tuple_page1 element + 1 (to min element if NULL), if unequal on DESC-> then set element to last_tuple_page1 element - 1, if the last_tuple_page1_element is not the min value, else set it to NULL
  * bplus_tree bulk loading
   * api to accept accept records, tuple by tuple
   * on process() call, generate interior pages (and returns root page id)
   * for building ith level of interior page, from the i-1th level, we copy the 1st row from the page with page id of the page, we will use the least_keys_page_id to link the level of the interior pages. for building tuple for iterior page level 1, we will use the suffix compression also to build the index tuple, while for other levels we will only use the same index tuple as in the first row of the interior page.
   * in the last pass, we will revisit all the interior pages using dfs and move the first row's page_id to least_keys_page_id and discard the first row.

* think about future project (THIS WILL BE PROJECTS THAT WILL DEPEND ON THIS PROJECT)
 * think about how to do undo and redo logging
 * logical logging with physiological logging will allow us higher concurrency
 * mini transactions like that innodb can generate very large log records, which we may not want in memory
   * clubbing all physiological logs for a logical operation into a single buffer and log them only upon completion (with strict 2pl on the latches), can use large amount of memory for log buffer, but you can easily redo and undo it, it is never undone physiologically, only logically
 * How about we use logical mini transaction redo, composed of physiological logs, now we do physiological undo when logical operation is incomplete, and logical undo for other operations above. we may have to undo and undo in this case
