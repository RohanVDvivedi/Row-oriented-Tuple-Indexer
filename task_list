** PICKED TASKS

VALIDATING WAS_MODIFIED BIT
 * implement an error handling strategy to check the was_modified bit, in in-memory-data-store, and exit with error, if the page was modified and was_modified was not set.

UPDATE-INSERT
 * discard all infra structure that was built to allow duplicates in the bplus_tree, there must only be preceeding_equals to get a child in interior page, no TOWARDS_FIRST and LAST allowed
 * first attempt is to segregate insert and delete into functions that can be easily reused
 * build single find_and_modify method supports all of inserr, update and delete
 * insert method first read couples to the bottom of the tree, and write locks only the leaf page
 * it searches the page for a record that compares equal to the new_record (both having same key value)
 * struct insert_inspect {
    const void* context;
    int (*inspect_insertion)(const void* context, const void* old_record, void** new_record);
    // old_record will be NULL, if not found
    // you can use the above function, to even modify the new_record based on the values of the old_record
    // setting the new_record to NULL, will delete the old_record
 };
 * once we have the write lock on the leaf page, we inspect it, using the above structure
 * if return value is 1
 * then we again match the key of old_record and new_record (if both new_record and old_record exist), to confirm that they match, the user could have made a mistake, while inspecting the new_record
 * if the old_record is NULL and new_record is NULL -> Do nothing by default
   if the old_record is not NULL and new_record is NULL -> Delete
   if the old_record is NULL and new_record is not NULL -> Insert
   if the old_record is not NULL and new_record is not NULL -> Update
 * then we check if the operation can be performed without splits and merges, if so do it and leave
 * for delete -> delete the old row, we only need to merge, so walk down take locks appropriately, perform merge loop function
 * for insert -> we only need to split, pass the new_tuple to split loop function
 * for update -> old_record  size == new_record size -> do nohing,
 -> old_record size < new_record size, delete old_record, try to insert the new_record, if split required, call split loop,
 -> old_record size > new_record_size, delete old_record, insert new_record, if the page is less than half full, call merge loop
 * for this we need separate functions to use the locked_pages stack to split and merge, up the tree

TRANSACTION_CONTEXT task
 * locks to the same transaction must pass, but WRITE latches to same page from different threads, even for same transaction must wait
 * this struct must be thread safe, using locks internally, while marking and reading abort related situation
 * struct transaction_abort_manager {
    void* transaction_handle; // user defined type to represent the transaction
    const transaction_abort_manager_methods* tamm_p;
 };
 struct transaction_abort_manager_methods{
    const void* context;
    int is_aborted(const void* transaction_id, const void* context); // returns 0 if not aborted, else returns reason for the abort
    void mark_aborted(const void* transaction_id, const void* context, int reason); // abort the transaction, with a reason for abort
 };
 * make an opaque_structs for transaction_abort_manager_methods and use only below methods in the TupleIndexer project
 *  int is_aborted(const transaction_abort_manager* tamm_p);
    void mark_aborted(const transaction_abort_manager* tamm_p, int reason);
 * mark_aborted appends a abort log entry and flushes the WAL
 * make all pmm, dam methods to take transaction_abort_manager as the input parameter (pmm may fail, if the system is out of memeory i.e. when malloc fails, WAL log is considered unfailable, if WAL fails then you don't even have a database, just exit the process in your pmm function implementations)
 * make all persistent_page methods (that are wrappers to pmm and dam) to take transaction_abort_manager as input parameter, and pass respective parameter down, to wrapped methods
 * for an aborted transaction anywhere in TupleIndexer, latches will be released by the TupleIndexer using the dam, but locks must be released after transaction completes
 * after each call to dam method wrappers for persistent_page, check if the transaction is aborted, if so release all locks and exit.

DATA_ACCESS_METHODS with tuple level locking
 * allow data_access_methods (with a context) to also enable locking only tuples on the page
 * function check if a tuple on a page is locked

ERROR RETURN CODES for DATA_ACCESS_METHODS
 * suggesting reason of a failure of data_access_method functions
 * reasons being DEADLOCK_HANDLED, CAN_T_LOCK_FREE_PAGE, OUT_OF_DISK_SPACE, OUT_OF_FREE_BUFFERS

** OPTIMIZATION
 * implement redistribute keys functions for fixed length index_def, this will reduce propogation of merges, this task can be delayed to be done at the end
 * see how this can be accomodated in the delete and update functions available at that point in future

** NECESSARY BUT DELAYED UNTIL ACTUAL REQUIREMENTS ARE FULFILLED
 * go through the project and implemnt better error handling for failures of malloc and data_access_methods
 * write multi-threaded test cases to insert a large csv in to b+tree, measure performance difference

** OTHER DATABASE DATASTRUCTURE TASKS (VERY IMPORTANT FOR FUTURE OF PROJECT)
 * introduce concept of temporary page, any changes to temporary pages are not logges to WAL except for the WAL logging for its allocate and free calls, their last updated log sequence number is set to their allocated log_sequence_number, and they are meant to be discarded soon after the transaction ends done and tracked by you during the transaction, on abort they are only to be freed as an undo operation, their contents are not to be persisted, they are to be used as intermediate query results. there has to be a bit in persistent_page stating they they are temporary pages. AGAIN no changes to these pages are not to be logged logged to WAL, but they are allowed to be persisted to disk. Since these pages are intermediate result pages, we dont care if their contents are lost upon a crash, in that case we abort the on going transaction on reboot, and then we are only required to free them, as written in WAL. In this case we are assuming a bit map based memeory allocator, that does not store any information on the page regarding its allocation state (like what a free list would do).
   * only WAL logs to allocate and deallocate a temporary page are persisted to the WAL
   * new idea, lets allow dam to allocate page with a transaction local flag, and allow it to free the pages in case we forget to. There would already be a lock manager who can take care of it.
   * we will also let data structure's in-memeory structs (like bpttd for bplus_tree) store whether the pages it allocates are temporary or not.
   * additionally a lock manager is responsible to free them upon a commit or abort
   * upon commit, the WAL is reread and all such pages of a transaction are again freed, if they are not freed
 * linked_page_list push_tuple_at_head, push_tuple_at_tail, get_head_tuple, get_tail_tuple, pop_tuple_from_head and pop_tuple_from_tail as its access functions and an iterator from head to tail or the other way around. it will be a doubly linkedlist of tuples
 * all inserts to linked_page_list have a flag to PRESERVE_ROOT_PAGE_ID
 * hash_table, a linux page table like hashtable where each bucket points a linked_page_list, each working with PRESERVE_ROOT_PAGE_ID
 * sort that can be called on a linked_page_list or a hash_table
 * utility header function that reads page_type from common_page_header and helps get and set the next_page_id and prev_page_id
   * this can be used to build a generic iterator over linked list, bplus_tree leaf pages and interior pages
 * building a b+tree using a sorted linked_page_list, layer by layer
   * we will first ask for the sorted page double linked list, then build the bplus_tree level by level (level 0 being leaf page)
     * for building ith level of interior page, we copy the first tuple's key of all the lower level (i-1 level) pages, and build a singly linkedlist of interior pages composed of each index record pointing to the lower level page, we maintain only the heads of the 2 adjacent levels and a write iterator to the upper level and read iterator of the lower level.
     * since in the initial phase we are not storing the least_keys_page_id in the interior pages, we can use this interior page header field to link all pages in the same level as a singly linkedlist
     * for this we need to define an union {all_least_keys_page_id next_page_id}, to use the utility iterator to work.
 * prefix key compression -> store prefix compression for var_strings (for internal nodes only), working in the following ways
    0  ->  "ABCD"            "ABCD"
    1  ->  "ABDEF"           "2DEF"
    2  ->  "ABDFG"           "3FG"
    3  ->  "ABDH"       ->   "3H"
    4  ->  "ADEA"            "1DEA"
    5  ->  "ADEF"            "3F"
    6  ->  "ADEGH"           "3GH"
    each var string except the first 1 stores itself OR its difference from the (same column) var string in the previous row
    the difference is stores as a number (suggesting the common characters from the previous var string) followed by the characters that are different.

    to differentiate the prefix number from the other characters, we will always store them in element_def's size_specifier_prefix_size number of bytes after an invalid unicode character "0xFF 0xFF".

    Algorithm to construct the string at ith index
    compute the length of the var string, that is 3 + "G" 1 + "H" 1 = 5
    bytes_to_read = 5 stack = ""
    now start iterating until bytes_to_read > 0
    i == 6 -> bytes_to_read = 3 stack = "HG"
    i == 5 -> bytes_to_read = 3 stack = "HG"
    i == 4 -> bytes_to_read = 1 stack = "HGED"
    i == 3 -> bytes_to_read = 1 stack = "HGED"
    i == 2 -> bytes_to_read = 1 stack = "HGED"
    i == 1 -> bytes_to_read = 1 stack = "HGED"
    i == 0 -> bytes_to_read = 0 stack = "HGEDA" -> reverse "ADEGH"

    stack = ""
    bytes_to_read = 5
    i = 6
    while(bytes_to_read > 0)
    {
      common_char_count, rest_of_string = get_column_from_ith_row(i);
      new_bytes_to_read = min(bytes_to_read, common_char_count);
      if(new_bytes_to_read > bytes_to_read)
      {
        for(int i = (new_bytes_to_read - bytes_to_read) - 1; i >= 0; i--)
          stack.push(rest_of_string[i]);
      }
      bytes_to_read = new_bytes_to_read;
    }
    return stack.reverse();
