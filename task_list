** PICKED TASKS

* hash_table, a page_table where each entry points a linked_page_list
   * when a linked_page_list becomes all empty, then linked_page_list is destroyed and NULL_PAGE_ID is written to page_table fo that bucket
   * Use linear hashing for dynamic bucket management, let there be N buckets, then set page_table[N] = root_page_id
   * for bucket spliting and there be N buckets, then
     * expand by splitting bucket split_index(N) into buckets split_index(N) and N, using b = H % (2 ^ (flb2(N) + 1)), then N++ (also N must not be equal to UINT64_MAX)
   * for merging then merge split_index(N - 1) and (N - 1), into split_index(N - 1), then N-- (also N must not be 1)
   * ensure that merging/shrinking a hash_table of bucket_count 1, always fails
   * ensure that splitting a hash_table of bucket_count UINT64_MAX, always fails
   * insert_in_hash_table, find_in_hash_table -> find returns an iterator for tuples that may compare equal
   * hash_table_iterator includes a linked_page_list_iterator over a bucket's linked_page_list and a page_table_range_locker over the bucket pointer
   * this iterator allows update (complete tuple (key elements remain the same), or update_element (for non key elements)) and remove
   * insert will take a flag fail_on_duplicate_key, which if set, fails insertion, if a dupicate is found, else it will just insert at head
   * insert procedure for high concurrency
    * walk down the page_table using a whole range lock
    * figure out the right bucket to insert into
    * create a linked_page_list at the bucket if it does not already exists and start a write iterator on it
    * release range_locker
    * insert tuple at head OR (if fail_on_duplicate_key is set) iterate over it, ensure it does not have a duplicate and then insert
    * delete linked_page_list_iterator and exit
   * find procedure for high concurrency
    * find the respective entry, if it is NULL, hold lock on the page_table_range_locker and exit
    * else open a linked_page_list_iterator and release lock on the page_table_range_locker
   * hash_table_iterator
    * it has a union of page_table_range_locker and a linked_page_list_iterator
    * if the bucket is empty, we will only lock the page_table_range_locker
    * you are then allowed to get_tuple, update_tuple, delete_tuple and update_non_key_column, but not insert
    * on deleting the hash_table_iterator
      * if the pmm_p was supplied, and the linked_page_list of the bucket is empty, then
        * release the linked_page_list_iterator of the empty bucket
        * open range_locker for the concerned bucket (the key has to be stored on the hash_table_iterator), if the bucket is NULL, quit
        * if the bucket is not empty, then open linked_page_list_itearator in write mode.
        * ensure that it is empty
        * if empty, close the iterator, delete this linked_page_list
        * and set the page_table_range_locker entry to NULL, then quit
    * support get_next, get_prev, using the key that we stored in the iterator, and allow going from head to tail and tail to head
   * pick a function to hash into a 64 bit unsigned integers

** OTHER DATABASE DATASTRUCTURE TASKS (VERY IMPORTANT FOR FUTURE OF PROJECT)
 * sort
   * it accepts record by record,
   * puts tuples in a page, sorts them and inserts it into a page_table
   * after a process() call, it will then merge adjacent runs until the page_table contains only 1 entry
   * then discards the page_tuple and returns tuples 1 by 1 from the linked_page_list 1 by 1
 * bplus_tree bulk loading
   * api to accept accept records, tuple by tuple
   * on process() call, generate interior pages (and returns root page id)
   * for building ith level of interior page, from the i-1th level, we copy the 1st row from the page with page id of the page, we will use the least_keys_page_id to link the level of the interior pages. for building tuple for iterior page level 1, we will use the suffix compression also to build the index tuple, while for other levels we will only use the same index tuple as in the first row of the interior page.
   * in the last pass, we will revisit all the interior pages using dfs and move the first row's page_id to least_keys_page_id and discard the first row.
 * allow cloning, read-only bplus_tree_iterator, page_table_range_locker and linked_page_list_iterator
   * clone all read locks inside them, and make a new object
 * build a power_table structure,
   * to hold 64 64-bit integers, to quickly calculate power of a number N (>= 2).
   * where ith integer of the array, PowerTable is = N ^ (2 ^ i)
   * to fing N ^ x
   * res = 1
     * for(b = 0; b < 64; b++)
     *   res = res * (((x >> b) & 1ULL) ? PowerTable[b] : 1)
   * through out the algorithm take care of overflows, and return 0, if result overflows
   * check if you could do this algorithm with large_uint (* future)
   * implement get_max_exponent_for_power_to_overflow -> this is the exponent at which the entries_per_page ^ exponent overflows 64 bit integer -> store this in power_table as overflow_exponent i.e. is greater than equal to (2 ^ 64), use this function to calculate the max_page_table_height
   * use the above function to weed out the powers that we know for sure will overflow, in the get_power function
 * (OPTIMIZATION) power_table in page_table at max needs only 6 entries, we can optimize this (minimum entries per page = 2) ^ (2^6) overflows a 64 bit integer
 * (OPTIMIZATION) max_page_table_height = log(2^64) base 2 -> will always fit a 32 bit number, and level of the page is also a 32 bit number, hence this variable must be a 32 bit number.
   * in calculation for calculating the max_page_table_height, you may only binary search from 0 to 64, as we already know that 2^64, will always over flow a 64 bit integer and 2 is the minimum entries per page
 * get rid of memcpy and instead use memory_move functionality for cutlery
 * hide functions of *_tuple_definitions, that do not need ot be public. including the functionality like power_factor
 * {} initialize, i.e. zero initialize all the tuple_defs at the beginning of the init_*_tuple_defs, to avoid garbage free, when deinit_*_tuple_defs functions is called on a failure of the corresponding init_*_tuple_defs functions
 * Rtree will be supported
   * very similar to b+tree, n dimensional data can be searched, inserted, deleted, no least_keys_page_id required
   * splits and merges happen according to the minimal increase in interior node areas
   * searches walk down a tree, and maintain a stack, releasing locks only if only 1 entry is to be scannd in the interior node, next, prev and get_curr are supported, leafs may be acquired with write locks
   * inserts walk down only 1 path of the tree, trying to minimize the increased area
   * deleted walk down like a search would, and since we maintain a stack, a delete can be performed, which will destroy the stack releasing all locks
   * you need to define a rtree_tuple_definitions, that also has a function to get the hyper_rectangle for any of the records, which defines the minimum area hyper rectangle that the record falls into, as expected that allows you to store n dimensional lines, points etc.
   * hyper rectangle is an n dimensional rectangle.
   * all interior nodes store a hyper rectangle and a corresponding page_id, and atleast 2 entries must fit on a page, else rttd contruction fails. yet you can have any number of dimensions.
   * you can also specify any numeral datatype for dimension, as long as it is supported by the TupleStore.
   * need to think about how to calculate areas of the hyper rectangle with dimensions of different types, such that it fits a number and also does not violate precision requirements (getting areas is necessary for splitting and merging). think about using GNU MP for this purpose of getting area. BUT we can not store GNU MP number in the TupleStore. ALSO you can just use GNU MP for substractions of lower_bound and upper_bound to estimate splits and merges, but while constructing bounding boxes, you can revert to get_max(d0_lb_1, d0_lb_1), etc for computing the new bounding boxes.
   * fail insertions if the lower_bound and upper_bound of the get_hyper_rectangle do not follow <= comparison for any of the n dimensions.
   * build utility function to check that hyper rectangle does not have a negative dimension (i.e. lower_bound <= upper_bound), construct hyper_rectangle covering 2 or more hyper rectangles, and check if a hyper_rectangle intersects with another hyper_rectangle etc.
   * all queries take only hyper_rectangle keys for search and delete, while user takes a leaf record (this is why we need a function to build a hyper rectangle).
   * inserts and deletes can release page locks if it is deemed that split and merge would not propogate up the tree
   * accomodate any tuple that is atleast half the size of the leaf page, and interior page tuples would be fixed width with non NULL elements
   * keys will have a even count and all of numeral types, for ith dimension lower bound would be 2*i th key element and 2*i+1 will be upper bound, up to d dimentions (0 <= i < 2*d).
 * think about implementing more data structures like T-tree, Patricia Trie, etc

LEFT FOR LATER TASKS
DAM FEATURES (CONTIGUOUS ALOCATION and DEALLOCATION of pages) (Still think if you need this, with current hashtable design we have, we do not need this)
  * uint64_t allocate_contiguous_pages(uint64_t page_count), returning first page id of the allocated pages.
  * free_contiguous_pages will not be supported, since we need to be aware that the page must not be locked or waiting to be locked, while doing this, and so we would need a lot to do to undo in case if a page can not be freed, hence only allocate_contiguous_pages will be the only thing close to what we will support in dam api.

** OPTIMIZATION (THIS MAY NEVER MAKE IT TO THE PROJECT)
 * implement redistribute keys functions for fixed length index_def, this will reduce propogation of merges, this task can be delayed to be done at the end
 * prefix key compression -> store prefix compression for var_strings (for internal nodes only), working in the following ways
    0  ->  "ABCD"            "ABCD"
    1  ->  "ABDEF"           "2DEF"
    2  ->  "ABDFG"           "3FG"
    3  ->  "ABDH"       ->   "3H"
    4  ->  "ADEA"            "1DEA"
    5  ->  "ADEF"            "3F"
    6  ->  "ADEGH"           "3GH"
    each var string except the first 1 stores itself OR its difference from the (same column) var string in the previous row
    the difference is stores as a number (suggesting the common characters from the previous var string) followed by the characters that are different.

    to differentiate the prefix number from the other characters, we will always store them in element_def's size_specifier_prefix_size number of bytes after an invalid unicode character "0xFF 0xFF".

    Algorithm to construct the string at ith index
    compute the length of the var string, that is 3 + "G" 1 + "H" 1 = 5
    bytes_to_read = 5 stack = ""
    now start iterating until bytes_to_read > 0
    i == 6 -> bytes_to_read = 3 stack = "HG"
    i == 5 -> bytes_to_read = 3 stack = "HG"
    i == 4 -> bytes_to_read = 1 stack = "HGED"
    i == 3 -> bytes_to_read = 1 stack = "HGED"
    i == 2 -> bytes_to_read = 1 stack = "HGED"
    i == 1 -> bytes_to_read = 1 stack = "HGED"
    i == 0 -> bytes_to_read = 0 stack = "HGEDA" -> reverse "ADEGH"

    stack = ""
    bytes_to_read = 5
    i = 6
    while(bytes_to_read > 0)
    {
      common_char_count, rest_of_string = get_column_from_ith_row(i);
      new_bytes_to_read = min(bytes_to_read, common_char_count);
      if(new_bytes_to_read > bytes_to_read)
      {
        for(int i = (new_bytes_to_read - bytes_to_read) - 1; i >= 0; i--)
          stack.push(rest_of_string[i]);
      }
      bytes_to_read = new_bytes_to_read;
    }
    return stack.reverse();
 * OPTIMIZATION in suffix truncation :: handle cases if INT, UINT, LARGE_UINT, BIT_FIELD, in loop 1, if unequal on ASC-> then set element to last_tuple_page1 element + 1 (to min element if NULL), if unequal on DESC-> then set element to last_tuple_page1 element - 1, if the last_tuple_page1_element is not the min value, else set it to NULL

* think about future project (THIS WILL BE PROJECTS THAT WILL DEPEND ON THIS PROJECT)
 * think about how to do undo and redo logging
 * logical logging with physiological logging will allow us higher concurrency
 * mini transactions like that innodb can generate very large log records, which we may not want in memory
   * clubbing all physiological logs for a logical operation into a single buffer and log them only upon completion (with strict 2pl on the latches), can use large amount of memory for log buffer, but you can easily redo and undo it, it is never undone physiologically, only logically
 * How about we use logical mini transaction redo, composed of physiological logs, now we do physiological undo when logical operation is incomplete, and logical undo for other operations above. we may have to undo and undo in this case
