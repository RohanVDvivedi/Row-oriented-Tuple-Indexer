** PICKED TASKS

1
  * DEADLOCK condition identified, and needs to be fixed
   * if page locks are persisted between calls, (predominantly write locks on pages, are assumed here, as persisting read locks for these library does not make sense, it will just hamper performance by limiting concurrency), then certain operations may create deadlocks, like
     * delete_array_table_range_locker() and delete_page_table_range_locker(), which may descend down the tree again to free up the empty local root page, and its respective
     * delete_hash_table_iterator(), when locking a single bucket, and then this bucket becomes empty
     * segregate these into separate functions to avoid deadlocks, the above mentioned operations do not alter or break the functionality, they would just be NO-OP in case they are not executed.
     * Here we assume that only write page locks persists between calls, which is still upto the user's usage of this library.
     * As a general case, one mini-transaction should only consist of 1 iterator open/close OR 1 non-iterator operation.
       * I am stating these here because if you persist write page locks inside a minitransacton, then 2 operations may deadlock, if these include multiple tree downward traversals, over same or different datastructures.

** OTHER DATABASE DATASTRUCTURE TASKS (VERY IMPORTANT FOR FUTURE OF PROJECT)
 * spend a day on output of `grep -inr --include "*.c" -A 5 -B 5 ",\sabort_error);" ./src > tb` and ensure that all aborts are handled properly
 * convert malloc calls, where ever possible to stack allocations (with or without alloca calls)
 * bplus_tree new features
   * allow update, delete and possibly insert over a stacked WRITE_LOCK-ed iterator
   * update only if the keys match and insert only if the key falls within the range of the currently locked range -> need to figure out how to check the 2nd part
     * update succeeds even on READ_LOCK_INTERIOR_WRITE_LOCK_LEAF, if the new record is atleast greater than or equal to the old record in size
   * insert to also take a flag as input that says if the tuple with same key already exists
     * finds a found_index and inserts at the insertion index -> both needs to be found on tha page
   * this will eventually also allow duplicates -> think if we need them, as MySQL's secondary index has them
      * for this also remove failures to insert if a record with same key found, instead pass a flag suggesting if duplicates are allowed, a new duplicate will always be inserted after the last existing duplicate
 * bplus_tree optimization
   * allow releasing parent page read locks on the stacked iterator, such that all results fall between the range [key1, key2] -> need to figure out how to do this
     * this is only allowed in stacked iterator case of READ_LOCK and READ_LOCK_INTERIOR_WRITE_LOCK_LEAF
     * do ensure that the current key we are pointing to does fall between the range, else fail this operation
 * better return values throughout the project 0 or 1 doesnt fit all requirements
   * all errors will be < 0, and success will be 0
   * ATTEMPTED_MODIFICATION_WITH_READ_LOCKS
   * RECORD_DOES_NOT_FIT_SIZE
   * RECORD_DOES_NOT_MATCH_KEY
   * RECORD_DOES_NOT_MATCH_KEY_RANGE
   * RECORD_DUPLICATE_FOUND
   * INVALID_ITERATOR_POSITION
   * INVALID_ITERATOR
   * ABORTED
   * SUCCESS = 0
 * introduce a new mechanism in page_access_methods to allow taking a persistent write lock on the page on which we already have a write lock on, this will mostly work with iterators
 * introduce a new page type, reference page
   * it will be primarily store the reference to the root of the data structure
   * it's tuple_def in compressed form
   * its reference counter -> which if reaches 0, then the underlying data structure and the page get destroyed
   * you may also store statistic information like height, fanout, number of inserts/updates per second
   * since this page and its underlying root that it points to are both fixed, the referenceed root pointer may never be appointed
   * at times it can try to relocate the root page (needs to be done gracefully in case of linked_page_list, since it is circular and not a tree)
   * it can also be used to house locking information on the underlying data structure
 * implement a LARGE_BLOB/LARGE_STRING a read-only datastructure
   * that will be a singly linked list of unmodifiable pages containing only a single non-nullable fixed length STRING or BLOB element
   * the root page has a reference counter, and the data is all deleted if the reference counter reaches zero
   * this tuple_def will be precomputed and put inside the tuple_definitions
   * implement compare function on them, and a read only byte stream to iterate over it character by character
   * check if you could allow this complex type to be part of hash_table and bplus_tree (My answer would be no, as keys must fit on the page to be searchable)
 * implement heap_table (needs to be refined)
   * using array_table as a lookup, bucket_id->(page_id, free_space)
   * free_space will be a hint and may not represent the actual free space on the page
   * access methods
     * insert a tuple and get a (bucket_id, slot_id), you may provide a (bucket_id, slot_id) as hint
       * get read lock on the array_table and check if hint has enough space, by fetching a write lock on the page_id
       * then search for a entry with a read lock on the whole range, that has enough free space for this tuple, if found insert there
       * then take a write lock on the first unallotted bucket_id and insert a new (page_id, free_space), with the new tuple contained in there
         * with this write lock also update free space of all pages in that bucket range, also delete any entries that point to empty pages
     * delete a tuple for the given (bucket_id, slot_id)
       * returns 0, for failure, 1 for success, 2 if the page became empty
     * open a read/write iterator on the bucket_id or range of bucket_ids, do as we did with the hash_table, only keep the reader lock on the 
       * only get a reader lock on the array_table
       * if it is a single bucket, then array_table_range_locked is not even needed
       * you can do anything with the page that the iterator points to (if it is a write iterator), but not update insert, delete or update any entries
       * API
         * next_heap_table_iterator -> goes to next page
         * prev_heap_table_iterator -> goes to prev page
         * get_curr_bucket_id -> returns the current bucket_id that the iterator points to
         * get_curr_page -> returns persistent page to work with
     * vaccum a range of bucket_ids, take a flag that instructs if the bucket_range is strict (if flexible vaccum any super set of the buckets containing the provided bucket range), fix the free_space and discard entries pointing to empty pages, use this after a bulk insert/delete operations
 * implement redistribute keys functions for fixed length index_def, this will reduce propogation of merges, this task can be delayed to be done at the end
   * this will be supported only for fixed_length index entries
   * function will move only 1 leaf/index entry from left to right or right to left at a time, everything will be done in place
   * fails if the receiving page is out of space, easy peasy with unaltered and altered util functions
   * parent page will also be required and will be updated immediately in the call, all 3 pages, parent and 2 children will be provided as is
   * will fail, also if index entries are not fixed sized
   * separate function to be worked out for leaf and index pages
   * also we will call this, as rotation and we need functions to check if we can left and right rotate
 * make functions to get the maximum pages that will be locked at once for any of the TupleIndexer data structures and iterators
 * Rtree will be supported
   * very similar to b+tree, n dimensional data can be searched, inserted, deleted, no least_keys_page_id required
   * splits and merges happen according to the minimal increase in interior node areas
   * searches walk down a tree, and maintain a stack, releasing locks only if only 1 entry is to be scannd in the interior node, next, prev and get_curr are supported, leafs may be acquired with write locks
   * inserts walk down only 1 path of the tree, trying to minimize the increased area
   * deleted walk down like a search would, and since we maintain a stack, a delete can be performed, which will destroy the stack releasing all locks
   * you need to define a rtree_tuple_definitions, that also has a function to get the hyper_rectangle for any of the records, which defines the minimum area hyper rectangle that the record falls into, as expected that allows you to store n dimensional lines, points etc.
   * hyper rectangle is an n dimensional rectangle.
   * all interior nodes store a hyper rectangle and a corresponding page_id, and atleast 2 entries must fit on a page, else rttd contruction fails. yet you can have any number of dimensions.
   * you can also specify any numeral datatype for dimension, as long as it is supported by the TupleStore.
   * need to think about how to calculate areas of the hyper rectangle with dimensions of different types, such that it fits a number and also does not violate precision requirements (getting areas is necessary for splitting and merging). think about using GNU MP for this purpose of getting area. BUT we can not store GNU MP number in the TupleStore. ALSO you can just use GNU MP for substractions of lower_bound and upper_bound to estimate splits and merges, but while constructing bounding boxes, you can revert to get_max(d0_lb_1, d0_lb_1), etc for computing the new bounding boxes.
   * fail insertions if the lower_bound and upper_bound of the get_hyper_rectangle do not follow <= comparison for any of the n dimensions.
   * build utility function to check that hyper rectangle does not have a negative dimension (i.e. lower_bound <= upper_bound), construct hyper_rectangle covering 2 or more hyper rectangles, and check if a hyper_rectangle intersects with another hyper_rectangle etc.
   * all queries take only hyper_rectangle keys for search and delete, while user takes a leaf record (this is why we need a function to build a hyper rectangle).
   * inserts and deletes can release page locks if it is deemed that split and merge would not propogate up the tree
   * accomodate any tuple that is atleast half the size of the leaf page, and interior page tuples would be fixed width with non NULL elements
   * keys will have a even count and all of numeral types, for ith dimension lower bound would be 2*i th key element and 2*i+1 will be upper bound, up to d dimentions (0 <= i < 2*d).

 * OPTIMIZATION in suffix truncation :: handle cases if INT, UINT, LARGE_UINT, BIT_FIELD, in loop 1, if unequal on ASC-> then set element to last_tuple_page1 element + 1 (to min element if NULL), if unequal on DESC-> then set element to last_tuple_page1 element - 1, if the last_tuple_page1_element is not the min value, else set it to NULL
