** PICKED TASKS

 * implement an error handling strategy to check the was_modified bit, in in-memory-data-store, and exit with error, if the page was modified and was_modified was not set.

 * print locks active count, on the close call of unWALed_in_memory_data_store
 * dam in release_lock(FREE_PAGE) -> allow the operation only if there is exactly only 1 thread, with reader and writer lock on the page, else fail to even release lock on the page
 * dam functions add functionality
  * uint64_t allocate_contiguous_pages(uint64_t page_count), returning first page id of the allocated pages.
  * free_contiguous_pages(uint64_t first_page_id, uint64_t page_count), instead of free_page -> to free n contiguous pages.
 * free_page must fail with an abort, if there are concurrent readers OR writers to the page, i.e. if a page is read or write locked (allow only 1 locker for the calling thread if FREE_PAGE is set for release_lock), while freeing a page.

** OTHER DATABASE DATASTRUCTURE TASKS (VERY IMPORTANT FOR FUTURE OF PROJECT)
 * hash_table, a linux page table like hashtable where each bucket points a linked_page_list, each working with PRESERVE_ROOT_PAGE_ID
   * each has table has 2 types of pages, HASH_ARRAY_PAGE and BUCKET_PAGE
   * BUCKET_PAGE are the ones that contain data, original tuples, with key and value (duplicate keys allowed)
   * HASH_ARRAY_PAGE are structured as tree, starting with level 0, and then on, like bplus tree
   * each entry in HASH_ARRAY_PAGE contain list of page_ids, each of page_id_width, pointing to another HASH_ARRAY_PAGE (when its level > 0) OR a BUCKET_PAGE (when its level = 0), all HASH_ARRAY_PAGE are identical.
   * root of the HASH_ARRAY_PAGE is never moved, root always stays at the same page, even on resizing.
   * If there are E entries in each of the HASH_ARRAY_PAGE, if the root page has level = n, then the hash table has E^(n+1) buckets.
   * If the root has only 1 entry at its first index, then the hash_table shrinks, by moving the contents of its new child to the root.
   * If any HASH_ARRAY_PAGE becomes all empty, then it is freed and an entry from its parent is discarded, shrinking the hash_table up the chain
   * HASH_ARRAY_PAGE for the children of other HASH_ARRAY_PAGE page are allocated, as and when required.
   * BUCKET_PAGE are organized as doubly linkedlist, head of which is pointed to by the leaves (level 0) of HASH_ARRAY_PAGE.
   * Original data tuples on BUCKET_PAGE are never moved, allowing to build heap organized tables, on top of hash_table
   * when a BUCKET_PAGE is all empty it is freed, and removed from the linkedlist, if it is the last page in the BUCKET_PAGE linkedlist, then a NULL is written at the corresponding entry of the HASH_ARRAY_PAGE
 * linked_page_list push_tuple_at_head, push_tuple_at_tail, get_head_tuple, get_tail_tuple, pop_tuple_from_head and pop_tuple_from_tail as its access functions and an iterator from head to tail or the other way around. it will be a doubly linkedlist of tuples
 * all inserts to linked_page_list have a flag to PRESERVE_ROOT_PAGE_ID
 * sort that can be called on a linked_page_list or a hash_table
 * utility header function that reads page_type from common_page_header and helps get and set the next_page_id and prev_page_id
   * this can be used to build a generic iterator over linked list, bplus_tree leaf pages and interior pages
 * building a b+tree using a sorted linked_page_list, layer by layer
   * we will first ask for the sorted page double linked list, then build the bplus_tree level by level (level 0 being leaf page)
     * for building ith level of interior page, we copy the first tuple's key of all the lower level (i-1 level) pages, and build a singly linkedlist of interior pages composed of each index record pointing to the lower level page, we maintain only the heads of the 2 adjacent levels and a write iterator to the upper level and read iterator of the lower level.
     * since in the initial phase we are not storing the least_keys_page_id in the interior pages, we can use this interior page header field to link all pages in the same level as a singly linkedlist
     * for this we need to define an union {all_least_keys_page_id next_page_id}, to use the utility iterator to work.

** OPTIMIZATION
 * implement redistribute keys functions for fixed length index_def, this will reduce propogation of merges, this task can be delayed to be done at the end
 * prefix key compression -> store prefix compression for var_strings (for internal nodes only), working in the following ways
    0  ->  "ABCD"            "ABCD"
    1  ->  "ABDEF"           "2DEF"
    2  ->  "ABDFG"           "3FG"
    3  ->  "ABDH"       ->   "3H"
    4  ->  "ADEA"            "1DEA"
    5  ->  "ADEF"            "3F"
    6  ->  "ADEGH"           "3GH"
    each var string except the first 1 stores itself OR its difference from the (same column) var string in the previous row
    the difference is stores as a number (suggesting the common characters from the previous var string) followed by the characters that are different.

    to differentiate the prefix number from the other characters, we will always store them in element_def's size_specifier_prefix_size number of bytes after an invalid unicode character "0xFF 0xFF".

    Algorithm to construct the string at ith index
    compute the length of the var string, that is 3 + "G" 1 + "H" 1 = 5
    bytes_to_read = 5 stack = ""
    now start iterating until bytes_to_read > 0
    i == 6 -> bytes_to_read = 3 stack = "HG"
    i == 5 -> bytes_to_read = 3 stack = "HG"
    i == 4 -> bytes_to_read = 1 stack = "HGED"
    i == 3 -> bytes_to_read = 1 stack = "HGED"
    i == 2 -> bytes_to_read = 1 stack = "HGED"
    i == 1 -> bytes_to_read = 1 stack = "HGED"
    i == 0 -> bytes_to_read = 0 stack = "HGEDA" -> reverse "ADEGH"

    stack = ""
    bytes_to_read = 5
    i = 6
    while(bytes_to_read > 0)
    {
      common_char_count, rest_of_string = get_column_from_ith_row(i);
      new_bytes_to_read = min(bytes_to_read, common_char_count);
      if(new_bytes_to_read > bytes_to_read)
      {
        for(int i = (new_bytes_to_read - bytes_to_read) - 1; i >= 0; i--)
          stack.push(rest_of_string[i]);
      }
      bytes_to_read = new_bytes_to_read;
    }
    return stack.reverse();