** PICKED TASKS

 * review worm.c functions

 * implement work_append_iterator
 * implement worm_read_iterator

 * simple function to get dependent_root_page_id of the worm
 * devise logic and api to destroy the dependent root of the worm, upon calling the decrement reference counter function
 * devise logic and api to return some value that suggests if whether reference count was decremented OR the whole worm was destroyed

 * WORM data structure
   * worm_append_iterator.c/.h
    * structure
      {
        persistent_page head_page;

        worm_tuple_defs* wtd_p ;
        page_access_methods* pam_p;
        page_modification_methods* pmm_p;
      }
    * worm_append_iterator* get_new_worm_append_iterator(uint64_t head_page_id, const page_access_methods* pam_p, const page_modification_methods* pmm_p, const void* transaction_id, int* abort_error); // keeps head page only locked and takes lock on tail page only on append call
    * int append_to_worm(worm_append_iterator* wai, const char* data, uint32_t data_size, const void* transaction_id, int* abort_error); // you can append at most 4GB at once
    * worm_append_iterator* delete_worm_append_iterator((worm_append_iterator* wai, const void* transaction_id, int* abort_error);
   * worm_read_iterator.c/.h
    * structure
      {
        persistent_page curr_page;

        uint32_t blob_index; // index of blob tuple on the current page
        uint32_t blob_offset; // offset in that blob for the byte to be read next

        worm_tuple_defs* wtd_p ;
        page_access_methods* pam_p;
      }
    * worm_read_iterator* get_new_worm_read_iterator(uint64_t head_page_id, const page_access_methods* pam_p, const void* transaction_id, int* abort_error); // keeps only the current page locked
    * worm_read_iterator* clone_worm_read_iterator(worm_read_iterator* wri, const void* transaction_id, int* abort_error);
    * int is_at_end_of_worm(worm_read_iterator* wri); // if the blob_index is equal to tuple index on the curr_page, blob_offset is 0, and there is no next page to go to
    * uint64_t seek_relative_in_worm(worm_read_iterator* wri, uint64_t bytes_to_skip, const void* transaction_id, int* abort_error); // you can only go forward, returns bytes seeked
    * uint32_t read_from_worm(worm_read_iterator* wri, int seek_forward, char* data, uint32_t data_size, const void* transaction_id, int* abort_error); // if the seek_forward flag is set, bytes_read are seeked forward
    * worm_read_iterator* delete_worm_read_iterator(worm_read_iterator* wri, const void* transaction_id, int* abort_error);
      * the append and read functions deal with uint32_t data_size, to force the user to pursue small sized buffers and rely more on streaming algorithms

** OTHER DATABASE DATASTRUCTURE TASKS (VERY IMPORTANT FOR FUTURE OF PROJECT)
 * memory_move for the set_*_page_header call is surely useless, removing it may reduce latency
   * this further must be done with not allowing any inheritence/compositions of the page_header-s
   * you must also remove the set from the common page header so it is never called again by any one, corrupting the header
   * think if this (non extensibility/hardcoding) is what you want
 * implement heap_table (needs to be refined)
   * using array_table as a lookup, bucket_id->(page_id, free_space)
   * free_space will be a hint and may not represent the actual free space on the page
   * access methods
     * insert a tuple and get a (bucket_id, slot_id), you may provide a (bucket_id, slot_id) as hint
       * get read lock on the array_table and check if hint has enough space, by fetching a write lock on the page_id
       * then search for a entry with a read lock on the whole range, that has enough free space for this tuple, if found insert there
       * then take a write lock on the first unallotted bucket_id and insert a new (page_id, free_space), with the new tuple contained in there
         * with this write lock also update free space of all pages in that bucket range, also delete any entries that point to empty pages
     * delete a tuple for the given (bucket_id, slot_id)
       * returns 0, for failure, 1 for success, 2 if the page became empty
     * open a read/write iterator on the bucket_id or range of bucket_ids, do as we did with the hash_table, only keep the reader lock on the 
       * only get a reader lock on the array_table
       * if it is a single bucket, then array_table_range_locked is not even needed
       * you can do anything with the page that the iterator points to (if it is a write iterator), but not update insert, delete or update any entries
       * API
         * next_heap_table_iterator -> goes to next page
         * prev_heap_table_iterator -> goes to prev page
         * get_curr_bucket_id -> returns the current bucket_id that the iterator points to
         * get_curr_page -> returns persistent page to work with
     * vaccum a range of bucket_ids, take a flag that instructs if the bucket_range is strict (if flexible vaccum any super set of the buckets containing the provided bucket range), fix the free_space and discard entries pointing to empty pages, use this after a bulk insert/delete operations
 * Rtree will be supported
   * very similar to b+tree, n dimensional data can be searched, inserted, deleted, no least_keys_page_id required
   * splits and merges happen according to the minimal increase in interior node areas
   * searches walk down a tree, and maintain a stack, releasing locks only if only 1 entry is to be scannd in the interior node, next, prev and get_curr are supported, leafs may be acquired with write locks
   * inserts walk down only 1 path of the tree, trying to minimize the increased area
   * deleted walk down like a search would, and since we maintain a stack, a delete can be performed, which will destroy the stack releasing all locks
   * you need to define a rtree_tuple_definitions, that also has a function to get the hyper_rectangle for any of the records, which defines the minimum area hyper rectangle that the record falls into, as expected that allows you to store n dimensional lines, points etc.
   * hyper rectangle is an n dimensional rectangle.
   * all interior nodes store a hyper rectangle and a corresponding page_id, and atleast 2 entries must fit on a page, else rttd contruction fails. yet you can have any number of dimensions.
   * you can also specify any numeral datatype for dimension, as long as it is supported by the TupleStore.
   * need to think about how to calculate areas of the hyper rectangle with dimensions of different types, such that it fits a number and also does not violate precision requirements (getting areas is necessary for splitting and merging). think about using GNU MP for this purpose of getting area. BUT we can not store GNU MP number in the TupleStore. ALSO you can just use GNU MP for substractions of lower_bound and upper_bound to estimate splits and merges, but while constructing bounding boxes, you can revert to get_max(d0_lb_1, d0_lb_1), etc for computing the new bounding boxes.
   * fail insertions if the lower_bound and upper_bound of the get_hyper_rectangle do not follow <= comparison for any of the n dimensions.
   * build utility function to check that hyper rectangle does not have a negative dimension (i.e. lower_bound <= upper_bound), construct hyper_rectangle covering 2 or more hyper rectangles, and check if a hyper_rectangle intersects with another hyper_rectangle etc.
   * all queries take only hyper_rectangle keys for search and delete, while user takes a leaf record (this is why we need a function to build a hyper rectangle).
   * inserts and deletes can release page locks if it is deemed that split and merge would not propogate up the tree
   * accomodate any tuple that is atleast half the size of the leaf page, and interior page tuples would be fixed width with non NULL elements
   * keys will have a even count and all of numeral types, for ith dimension lower bound would be 2*i th key element and 2*i+1 will be upper bound, up to d dimentions (0 <= i < 2*d).
 * build functions to relocate root of the datastructures to lower page ids

FAR FUTURE TASKS AND CONCEPTS
  * implement borrow_from_left and borrow_from_right functions for interior pages and leaf pages to be performed while merges
   * this will be implemented for variable length keys aswell
   * this will take parent page, left page and right page as input and will perform a borrow to make left and right pages balanced if it is possible by the use of borrowing
   * since this function will also modify the parent page, do relese lock on the parent page also if this operation succeeds
   * these functions will be called inside the merge_and unlock_pages_up right before we attempt to merge
   * if the borrowing succeeds we can directly release locks on all the three pages, pop the parent from the locked pages stack and break out of this loop
 * OPTIMIZATION in suffix truncation :: handle cases if INT, UINT, LARGE_UINT, BIT_FIELD, in loop 1, if unequal on ASC-> then set element to last_tuple_page1 element + 1 (to min element if NULL), if unequal on DESC-> then set element to last_tuple_page1 element - 1, if the last_tuple_page1_element is not the min value, else set it to NULL
